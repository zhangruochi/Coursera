{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi fare Keras Feature Columns (playground).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/08_Taxifare_Keras_FeatureColumns_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fIvCMy8TH170",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras Feature Columns are not an officially released feature yet. Some caveats apply: please run this notebook on a GPU Backend. Keras Feature Columns are not comaptible with TPUs yet. Also, you will not be able to export this model to Tensorflow's \"saved model\" format for serving. The serving layer for feature columns will be added soon.\n"
      ]
    },
    {
      "metadata": {
        "id": "8g9DaHx7HFtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "4RmX7O1HBoFE",
        "colab_type": "code",
        "outputId": "f5e299ac-caa9-4723-dbac-31debb77faa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os, json, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.feature_column import feature_column_v2 as fc  # This will change when Keras FeatureColumn is final.\n",
        "from matplotlib import pyplot as plt\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k4565fhT0hRn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title display utilities [RUN ME]\n",
        "# utility to display training and validation curves\n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBn9AycUKk2U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab-only auth"
      ]
    },
    {
      "metadata": {
        "id": "RTVUYJiZKoRc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# backend identification\n",
        "IS_COLAB = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "HAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "# Auth on Colab\n",
        "if IS_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  \n",
        "# Also propagate the Auth to TPU if available so that it can access your GCS buckets\n",
        "if IS_COLAB and HAS_COLAB_TPU:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "  with tf.Session(TF_MASTER) as sess:    \n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f) # Upload the credentials to TPU.\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "  print('Using TPU')\n",
        "\n",
        "# TPU usage flag\n",
        "USE_TPU = HAS_COLAB_TPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OXx6ozUHzPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Config"
      ]
    },
    {
      "metadata": {
        "id": "asbCdV0iH1z8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_BUCKET = \"gs://cloud-training-demos/taxifare/\"\n",
        "TRAIN_DATA_PATTERN = DATA_BUCKET + \"train*\"\n",
        "VALID_DATA_PATTERN = DATA_BUCKET + \"valid*\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EuDURqOPJiQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CSV_COLUMNS = ['fare_amount', 'dayofweek', 'hourofday', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
        "DEFAULTS = [[0.0], ['null'], [12], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
        "\n",
        "def decode_csv(line):\n",
        "  column_values = tf.decode_csv(line, DEFAULTS)\n",
        "  column_names = CSV_COLUMNS\n",
        "  decoded_line = dict(zip(column_names, column_values)) # create a dictionary {'column_name': value, ...} for each line  \n",
        "  return decoded_line\n",
        "\n",
        "def load_dataset(pattern):\n",
        "  #filenames = tf.gfile.Glob(pattern)\n",
        "  filenames = tf.data.Dataset.list_files(pattern)\n",
        "  #dataset = tf.data.TextLineDataset(filenames)\n",
        "  dataset = filenames.interleave(tf.data.TextLineDataset, cycle_length=16)  # interleave so that reading happens from multiple files in parallel\n",
        "  dataset = dataset.map(decode_csv)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSleDYsrNXzs",
        "colab_type": "code",
        "outputId": "06b0cd25-031e-436d-ebe0-2dd335e9c858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(TRAIN_DATA_PATTERN)\n",
        "for n, data in enumerate(dataset):\n",
        "  numpy_data = {k: v.numpy() for k, v in data.items()} # .numpy() works in eager mode\n",
        "  print(numpy_data)\n",
        "  if n>10: break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fare_amount': 9.7, 'dayofweek': b'Wed', 'hourofday': 9, 'pickuplon': -73.958984, 'pickuplat': 40.767925, 'dropofflon': -73.977974, 'dropofflat': 40.74173, 'passengers': 1.0, 'key': b'2010-04-14 09:39:00.000000-73.95940.767940.7417-73.978'}\n",
            "{'fare_amount': 24.9, 'dayofweek': b'Fri', 'hourofday': 23, 'pickuplon': -73.977684, 'pickuplat': 40.763264, 'dropofflon': -73.97954, 'dropofflat': 40.66857, 'passengers': 4.0, 'key': b'2010-01-22 23:48:20.000000-73.977740.763340.6686-73.9795'}\n",
            "{'fare_amount': 9.3, 'dayofweek': b'Fri', 'hourofday': 19, 'pickuplon': -73.994225, 'pickuplat': 40.719498, 'dropofflon': -73.97495, 'dropofflat': 40.744637, 'passengers': 3.0, 'key': b'2010-12-17 19:35:00.000000-73.994240.719540.7446-73.975'}\n",
            "{'fare_amount': 16.5, 'dayofweek': b'Fri', 'hourofday': 16, 'pickuplon': -73.98174, 'pickuplat': 40.749756, 'dropofflon': -74.00382, 'dropofflat': 40.737328, 'passengers': 1.0, 'key': b'2014-04-25 16:43:00.000000-73.981740.749840.7373-74.0038'}\n",
            "{'fare_amount': 5.7, 'dayofweek': b'Sun', 'hourofday': 0, 'pickuplon': -73.985016, 'pickuplat': 40.7476, 'dropofflon': -74.000786, 'dropofflat': 40.742096, 'passengers': 5.0, 'key': b'2011-03-13 00:59:00.000000-73.98540.747640.7421-74.0008'}\n",
            "{'fare_amount': 4.9, 'dayofweek': b'Wed', 'hourofday': 18, 'pickuplon': -73.96605, 'pickuplat': 40.76223, 'dropofflon': -73.95454, 'dropofflat': 40.76722, 'passengers': 1.0, 'key': b'2009-12-23 18:30:41.000000-73.96640.762240.7672-73.9545'}\n",
            "{'fare_amount': 11.3, 'dayofweek': b'Tue', 'hourofday': 18, 'pickuplon': -73.97075, 'pickuplat': 40.750977, 'dropofflon': -74.003, 'dropofflat': 40.724342, 'passengers': 4.0, 'key': b'2011-04-19 18:24:04.000000-73.970740.75140.7243-74.003'}\n",
            "{'fare_amount': 15.0, 'dayofweek': b'Tue', 'hourofday': 8, 'pickuplon': -73.96782, 'pickuplat': 40.755478, 'dropofflon': -73.99326, 'dropofflat': 40.747993, 'passengers': 6.0, 'key': b'2014-08-05 08:21:00.000000-73.967840.755540.748-73.9933'}\n",
            "{'fare_amount': 30.1, 'dayofweek': b'Tue', 'hourofday': 21, 'pickuplon': -73.870766, 'pickuplat': 40.77385, 'dropofflon': -73.97459, 'dropofflat': 40.761505, 'passengers': 1.0, 'key': b'2011-10-04 21:16:25.000000-73.870840.773840.7615-73.9746'}\n",
            "{'fare_amount': 12.5, 'dayofweek': b'Fri', 'hourofday': 14, 'pickuplon': -73.97276, 'pickuplat': 40.7889, 'dropofflon': -73.96555, 'dropofflat': 40.78649, 'passengers': 1.0, 'key': b'2010-10-08 14:52:00.000000-73.972840.788940.7865-73.9655'}\n",
            "{'fare_amount': 14.5, 'dayofweek': b'Tue', 'hourofday': 11, 'pickuplon': -73.94787, 'pickuplat': 40.783154, 'dropofflon': -73.99109, 'dropofflat': 40.739613, 'passengers': 2.0, 'key': b'2011-06-21 11:03:10.000000-73.947940.783240.7396-73.9911'}\n",
            "{'fare_amount': 5.3, 'dayofweek': b'Tue', 'hourofday': 14, 'pickuplon': -73.964554, 'pickuplat': 40.764713, 'dropofflon': -73.96179, 'dropofflat': 40.774384, 'passengers': 2.0, 'key': b'2010-11-30 14:51:00.000000-73.964640.764740.7744-73.9618'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xYZzTbmAVTWn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_engineered(features):\n",
        "    # this is how you can do feature engineering in TensorFlow\n",
        "    distance = tf.sqrt((features['pickuplat'] - features['dropofflat'])**2 +\n",
        "                       (features['pickuplon'] - features['dropofflon'])**2)\n",
        "    \n",
        "    # euclidian distance is hard for a neural network to emulate\n",
        "    features['euclidean'] = distance\n",
        "    return features\n",
        "\n",
        "def features_and_labels(features):\n",
        "  features = add_engineered(features)\n",
        "  features.pop('key') # this column not needed\n",
        "  label = features.pop('fare_amount') # this is what we will train for\n",
        "  return features, label\n",
        "  \n",
        "def prepare_dataset(dataset, batch_size, truncate=None, shuffle=True):\n",
        "  dataset = dataset.map(features_and_labels)\n",
        "  if truncate is not None:\n",
        "    dataset = dataset.take(truncate)\n",
        "  dataset = dataset.cache()\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(-1) # prefetch next batch while training  (-1: autotune prefetch buffer size)\n",
        "  return dataset\n",
        "\n",
        "one_item = load_dataset(TRAIN_DATA_PATTERN).map(features_and_labels).take(1).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9Zf5ra8WZk_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Keras model [WORK REQUIRED]\n",
        "1. What do the columns do ? Familiarize yourself with these column types.\n",
        "\n",
        "`numeric_col = fc.`[`numeric_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)`('name')`\n",
        "\n",
        "`bucketized_numeric_col = fc.`[`bucketized_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column)`(fc.`[`numeric_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)`('name'), [0, 2, 10])`\n",
        "\n",
        "\n",
        "`indic_of_categ_col = fc.`[`indicator_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)`(fc.`[`categorical_column_with_identity`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity)`('name', num_buckets = 24))`\n",
        "\n",
        "`indic_of_categ_vocab_col = fc.`[`indicator_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)`(fc.`[`categorical_column_with_identity`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)`('color', vocabulary_list = ['red', 'blue']))` \n",
        "\n",
        "\n",
        "`indic_of_crossed_col = fc.`[`indicator_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)`(fc.`[`crossed_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column)`([categcol1, categcol2], 16*16))`\n",
        "\n",
        "\n",
        "`embedding_of_crossed_col = fc.`[`embedding_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)`(fc.`[`crossed_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column)`([categcol1, categcol2], 16*16), 5)`\n",
        "\n",
        "| column | output vector shape | nb of parameters |\n",
        "|--------------|---------------------------------|------------------------------|\n",
        "| numeric_col | [1] | 0 |\n",
        "| bucketized_numeric_col | [bucket boundaries+1] | 0 |\n",
        "| indic_of_categ_col | [nb categories] | 0 |\n",
        "| indic_of_categ_vocab_col | [nb categories] | 0 |\n",
        "| indic_of_crossed_col | [nb crossed categories] | 0 |\n",
        "| embedding_of_crossed_col | [nb crossed categories] | crossed categories * embedding size |\n",
        "\n",
        "2. Let's start with all the data in as simply as possible: numerical columns for numerical values, categorical (one-hot encoded) columns for categorical data like the day of the week or the hour of the day. Try training...\n",
        " * RSME flat at 8-9 ... not good\n",
        "3. Try to replace the numerical latitude and longitudes by their bucketized versions\n",
        " * RSME trains to 6 ... progress!\n",
        "4. Try to add an engineered feature like 'euclidean' for the distance traveled by the taxi\n",
        " * RMSE trains down to 4-5 ... progress !\n",
        " The euclidian distance is really hard to emulate for a neural network. Look through the code to see how it was \"engineered\".\n",
        "5. Now add embedded crossed columns for:\n",
        "  * hourofday x dayofweek\n",
        "   * pickup neighborhood (bucketized pickup lon x bucketized pickup lat)\n",
        "   * dropoff neighborhood (bucketized dropoff lon x bucketized dropoff lat)\n",
        "   * is this better ?\n",
        "   \n",
        "The big wins were bucketizing the coordinates and adding the euclidian distance. The cross column add only a little, and only if you train for longer. Try training on 10x the training and validation data. With crossed columns you should be able to reach RMSE=3.9"
      ]
    },
    {
      "metadata": {
        "id": "F0qPAxfnWYaj",
        "colab_type": "code",
        "outputId": "2372913c-d4b7-4a3c-ed59-9c21d7cb180e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "NB_BUCKETS = 16\n",
        "latbuckets = np.linspace(38.0, 42.0, NB_BUCKETS).tolist()\n",
        "lonbuckets = np.linspace(-76.0, -72.0, NB_BUCKETS).tolist()\n",
        "\n",
        "\n",
        "# the columns you can play with\n",
        "\n",
        "# Categorical columns are used as:\n",
        "# fc.indicator_column(dayofweek)\n",
        "dayofweek = fc.categorical_column_with_vocabulary_list('dayofweek', vocabulary_list = ['Sun', 'Mon', 'Tues', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
        "hourofday = fc.categorical_column_with_identity('hourofday', num_buckets = 24)\n",
        "\n",
        "# Bucketized columns can be used as such:\n",
        "bucketized_pick_lat = fc.bucketized_column(fc.numeric_column('pickuplon'), lonbuckets)\n",
        "bucketized_pick_lon = fc.bucketized_column(fc.numeric_column('pickuplat'), latbuckets)\n",
        "bucketized_drop_lat = fc.bucketized_column(fc.numeric_column('dropofflon'), lonbuckets)\n",
        "bucketized_drop_lon = fc.bucketized_column(fc.numeric_column('dropofflat'), latbuckets)\n",
        "\n",
        "# Cross columns are used as\n",
        "# fc.embedding_column(day_hr, 5)\n",
        "day_hr =  fc.crossed_column([dayofweek, hourofday], 24 * 7)\n",
        "pickup_cross  = fc.crossed_column([bucketized_pick_lat, bucketized_pick_lon], NB_BUCKETS * NB_BUCKETS)\n",
        "drofoff_cross = fc.crossed_column([bucketized_drop_lat, bucketized_drop_lon], NB_BUCKETS * NB_BUCKETS)\n",
        "#pickdorp_pair  = fc.crossed_column([pickup_cross, ddropoff_cross], NB_BUCKETS ** 4 )\n",
        "  \n",
        "columns = [\n",
        "    \n",
        "    ###\n",
        "    #\n",
        "    # YOUR FEATURE COLUMNS HERE\n",
        "    #\n",
        "    fc.numeric_column('passengers'),\n",
        "    ##\n",
        "]\n",
        "\n",
        "l = tf.keras.layers\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        fc.FeatureLayer(columns),\n",
        "        l.Dense(100, activation='relu'),\n",
        "        l.Dense(64, activation='relu'),\n",
        "        l.Dense(32, activation='relu'),\n",
        "        l.Dense(16, activation='relu'),\n",
        "        l.Dense(1, activation=None), # regression\n",
        "    ])\n",
        "\n",
        "def rmse(y_true, y_pred): # Root Mean Squared Error\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "def mae(y_true, y_pred): # Mean Squared Error\n",
        "  return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "  \n",
        "model.compile(optimizer=tf.train.AdamOptimizer(), # little bug: in eager mode, 'adam' is not yet accepted, must spell out tf.train.AdamOptimizer()\n",
        "              loss='mean_squared_error',\n",
        "              metrics=[rmse])\n",
        "\n",
        "# print model layers\n",
        "model.predict(one_item, steps=1) # little bug: with FeatureLayer, must call the model once on dummy data before .summary can work\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "feature_layer_15 (FeatureLay multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             multiple                  200       \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             multiple                  6464      \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             multiple                  2080      \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             multiple                  528       \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             multiple                  17        \n",
            "=================================================================\n",
            "Total params: 9,289\n",
            "Trainable params: 9,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwlP-V2yulrl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 8\n",
        "BATCH_SIZE = 512\n",
        "TRAIN_SIZE = 64*1024  # max is 2,141,023 (1081336 on dataset in ch3)\n",
        "VALID_SIZE = 4*1024   # max is 2,124,500\n",
        "\n",
        "# Playground settings: TRAIN_SIZE = 64*1024, VALID_SIZE = 4*1024\n",
        "# Solution settings: TRAIN_SIZE = 640*1024, VALID_SIZE = 64*1024\n",
        "\n",
        "# This should reach RMSE = 3.9 (multiple runs may be necessary)\n",
        "\n",
        "train_dataset = prepare_dataset(load_dataset(TRAIN_DATA_PATTERN), batch_size=BATCH_SIZE, truncate=TRAIN_SIZE)\n",
        "valid_dataset = prepare_dataset(load_dataset(VALID_DATA_PATTERN), batch_size=BATCH_SIZE, truncate=VALID_SIZE, shuffle=False)\n",
        "\n",
        "history = model.fit(train_dataset, steps_per_epoch=TRAIN_SIZE//BATCH_SIZE, epochs=EPOCHS, shuffle=True,\n",
        "                    validation_data=valid_dataset, validation_steps=VALID_SIZE//BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5Ty-HFjWbPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['rmse'], history.history['val_rmse'], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}