{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Batch normalisation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSU8qu6mtXkn"
      },
      "source": [
        "# Batch normalisation layers\n",
        "\n",
        "In this reading we will look at incorporating batch normalisation into our models and look at an example of how we do this in practice.\n",
        "\n",
        "As usual, let's first import tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVPAUmP2tXkp"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VG0sN6OtXkw"
      },
      "source": [
        "We will be working with the diabetes dataset that we have been using in this week's screencasts. \n",
        "\n",
        "Let's load and pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Gt-7LRtXky"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes_dataset = load_diabetes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L2EYQXFtXk2"
      },
      "source": [
        "# Save the input and target variables\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = diabetes_dataset['data']\n",
        "targets = diabetes_dataset['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs9-_XX3tXk7"
      },
      "source": [
        "# Normalise the target data (this will make clearer training curves)\n",
        "\n",
        "targets = (targets - targets.mean(axis=0)) / (targets.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG8DX3W5tXk-"
      },
      "source": [
        "# Split the dataset into training and test datasets \n",
        "\n",
        "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPMsLm3ttXlA"
      },
      "source": [
        "### Batch normalisation - Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg7PaApltXlB"
      },
      "source": [
        "We can implement batch normalisation into our model by adding it in the same way as any other layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_fktNxtXlB"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyljY1EdtXlE"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=[train_data.shape[1],], activation=\"relu\"),\n",
        "    BatchNormalization(),  # <- Batch normalisation layer\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),  # <- Batch normalisation layer\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "])\n",
        "\n",
        "# NB: We have not added the output layer because we still have more layers to add!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "emoP_2bFtXlF"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StYn4ixEtXlH"
      },
      "source": [
        "Recall that there are some parameters and hyperparameters associated with batch normalisation.\n",
        "\n",
        "* The hyperparameter **momentum** is the weighting given to the previous running mean when re-computing it with an extra minibatch. By **default**, it is set to 0.99.\n",
        "\n",
        "* The hyperparameter **$\\epsilon$** is used for numeric stability when performing the normalisation over the minibatch. By **default** it is set to 0.001.\n",
        "\n",
        "* The parameters **$\\beta$** and **$\\gamma$** are used to implement an affine transformation after normalisation. By **default**, $\\beta$ is an all-zeros vector, and $\\gamma$ is an all-ones vector.\n",
        "\n",
        "### Customising parameters\n",
        "These can all be changed (along with various other properties) by adding optional arguments to `tf.keras.layers.BatchNormalization()`.\n",
        "\n",
        "We can also specify the axis for batch normalisation. By default, it is set as -1.\n",
        "\n",
        "Let's see an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJRdM2OtXlI"
      },
      "source": [
        "# Add a customised batch normalisation layer\n",
        "\n",
        "model.add(tf.keras.layers.BatchNormalization(\n",
        "    momentum=0.95, \n",
        "    epsilon=0.005,\n",
        "    axis = -1,\n",
        "    beta_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05), \n",
        "    gamma_initializer=tf.keras.initializers.Constant(value=0.9)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNYdiKmTtXlK"
      },
      "source": [
        "# Add the output layer\n",
        "\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfy-UOgftXlM"
      },
      "source": [
        "## Compile and fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhK66yLRtXlM"
      },
      "source": [
        "Let's now compile and fit our model with batch normalisation, and track the progress on training and validation sets.\n",
        "\n",
        "First we compile our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg6t9Ec7tXlM"
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PsLxbnztXlO"
      },
      "source": [
        "Now we fit the model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PY_2stdtXlO"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "history = model.fit(train_data, train_targets, epochs=100, validation_split=0.15, batch_size=64,verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS41StcNtXlQ"
      },
      "source": [
        "Finally, we plot training and validation loss and accuracy to observe how the accuracy of our model improves over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpnkUBSdtXlR"
      },
      "source": [
        "# Plot the learning curves\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "frame = pd.DataFrame(history.history)\n",
        "epochs = np.arange(len(frame))\n",
        "\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "\n",
        "# Loss plot\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(epochs, frame['loss'], label=\"Train\")\n",
        "ax.plot(epochs, frame['val_loss'], label=\"Validation\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.set_title(\"Loss vs Epochs\")\n",
        "ax.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(epochs, frame['mae'], label=\"Train\")\n",
        "ax.plot(epochs, frame['val_mae'], label=\"Validation\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Mean Absolute Error\")\n",
        "ax.set_title(\"Mean Absolute Error vs Epochs\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY1rKgRdtXlS"
      },
      "source": [
        "## Further reading and resources \n",
        "* https://keras.io/layers/normalization/\n",
        "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization"
      ]
    }
  ]
}